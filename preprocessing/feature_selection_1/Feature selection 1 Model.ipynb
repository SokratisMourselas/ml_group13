{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting data from csv files into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting data from csv files into numpy arrays\n",
    "from numpy import genfromtxt\n",
    "files_data=np.array(['fold_data_1.csv','fold_data_2.csv','fold_data_3.csv','fold_data_4.csv','fold_data_5.csv',\n",
    "                'fold_data_6.csv','fold_data_7.csv','fold_data_8.csv','fold_data_9.csv','fold_data_10.csv'])\n",
    "X_train_data= np.array([genfromtxt(file,delimiter=',') for file in files_data])\n",
    "\n",
    "files_label=(['fold_labels_1.csv','fold_labels_2.csv','fold_labels_3.csv','fold_labels_4.csv','fold_labels_5.csv',\n",
    "                'fold_labels_6.csv','fold_labels_7.csv','fold_labels_8.csv','fold_labels_9.csv','fold_labels_10.csv'])\n",
    "Y_train_labels= np.array([genfromtxt(file,delimiter=',') for file in files_label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(873, 240)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "Y_train=([to_categorical(Y_train_labels, num_classes=10) for Y_train_labels in Y_train_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping to 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1D=([np.reshape(X_train_data,(X_train_data.shape[0], 240, 1)) for X_train_data in X_train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(873, 60, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train_2D=([np.reshape(X_train_1D,(X_train_1D.shape[0], 60, 4)) for X_train_1D in X_train_1D])\n",
    "print(np.shape(X_train_2D[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=([np.reshape(X_train_2D,(X_train_2D.shape[0],60,4,1)) for X_train_2D in X_train_2D])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapes of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 = (873, 60, 4, 1)\n",
      "Fold 2 = (888, 60, 4, 1)\n",
      "Fold 3 = (925, 60, 4, 1)\n",
      "Fold 4 = (990, 60, 4, 1)\n",
      "Fold 5 = (936, 60, 4, 1)\n",
      "Fold 6 = (823, 60, 4, 1)\n",
      "Fold 7 = (838, 60, 4, 1)\n",
      "Fold 8 = (806, 60, 4, 1)\n",
      "Fold 9 = (816, 60, 4, 1)\n",
      "Fold 10 = (837, 60, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range((10)):\n",
    "    print(\"Fold\",i+1,\"=\",np.shape(X_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "873/873 [==============================] - ETA: 7s - loss: 205.2562 - accuracy: 0.02 - ETA: 2s - loss: 103.8928 - accuracy: 0.10 - ETA: 2s - loss: 89.8405 - accuracy: 0.1200 - ETA: 1s - loss: 90.3268 - accuracy: 0.130 - ETA: 0s - loss: 78.5722 - accuracy: 0.147 - ETA: 0s - loss: 92.4945 - accuracy: 0.142 - ETA: 0s - loss: 90.0579 - accuracy: 0.148 - ETA: 0s - loss: 88.7000 - accuracy: 0.150 - ETA: 0s - loss: 86.2566 - accuracy: 0.140 - ETA: 0s - loss: 82.3975 - accuracy: 0.141 - ETA: 0s - loss: 79.6777 - accuracy: 0.140 - ETA: 0s - loss: 80.8998 - accuracy: 0.150 - ETA: 0s - loss: 93.4595 - accuracy: 0.149 - 1s 1ms/step - loss: 91.4419 - accuracy: 0.1501\n",
      "Epoch 2/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 24.1107 - accuracy: 0.100 - ETA: 0s - loss: 78.9216 - accuracy: 0.160 - ETA: 0s - loss: 63.1224 - accuracy: 0.185 - ETA: 0s - loss: 63.7607 - accuracy: 0.190 - ETA: 0s - loss: 57.1130 - accuracy: 0.195 - ETA: 0s - loss: 51.8551 - accuracy: 0.196 - ETA: 0s - loss: 48.6356 - accuracy: 0.198 - ETA: 0s - loss: 44.8559 - accuracy: 0.205 - ETA: 0s - loss: 42.5693 - accuracy: 0.210 - ETA: 0s - loss: 41.5214 - accuracy: 0.211 - 1s 832us/step - loss: 40.9136 - accuracy: 0.2131\n",
      "Epoch 3/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 41.1250 - accuracy: 0.240 - ETA: 0s - loss: 157.9232 - accuracy: 0.23 - ETA: 0s - loss: 198.3715 - accuracy: 0.20 - ETA: 0s - loss: 242.6098 - accuracy: 0.20 - ETA: 0s - loss: 209.9386 - accuracy: 0.20 - ETA: 0s - loss: 172.4638 - accuracy: 0.21 - ETA: 0s - loss: 161.7155 - accuracy: 0.23 - ETA: 0s - loss: 143.1071 - accuracy: 0.22 - ETA: 0s - loss: 128.1136 - accuracy: 0.22 - ETA: 0s - loss: 116.8456 - accuracy: 0.21 - 1s 854us/step - loss: 116.7787 - accuracy: 0.2142\n",
      "Epoch 4/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 18.0935 - accuracy: 0.240 - ETA: 0s - loss: 58.2599 - accuracy: 0.166 - ETA: 0s - loss: 48.9596 - accuracy: 0.196 - ETA: 0s - loss: 74.4641 - accuracy: 0.205 - ETA: 0s - loss: 62.2893 - accuracy: 0.204 - ETA: 0s - loss: 56.8272 - accuracy: 0.207 - ETA: 0s - loss: 51.3301 - accuracy: 0.210 - ETA: 0s - loss: 45.9101 - accuracy: 0.216 - ETA: 0s - loss: 43.1534 - accuracy: 0.223 - 1s 758us/step - loss: 42.1804 - accuracy: 0.2211\n",
      "Epoch 5/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 20.5355 - accuracy: 0.160 - ETA: 0s - loss: 17.5132 - accuracy: 0.193 - ETA: 0s - loss: 20.0682 - accuracy: 0.204 - ETA: 0s - loss: 21.7895 - accuracy: 0.220 - ETA: 0s - loss: 24.5655 - accuracy: 0.220 - ETA: 0s - loss: 21.7200 - accuracy: 0.221 - ETA: 0s - loss: 25.9029 - accuracy: 0.223 - ETA: 0s - loss: 24.4312 - accuracy: 0.221 - ETA: 0s - loss: 23.0554 - accuracy: 0.230 - 1s 767us/step - loss: 22.6769 - accuracy: 0.2279\n",
      "Epoch 6/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 5.0279 - accuracy: 0.28 - ETA: 0s - loss: 8.1187 - accuracy: 0.26 - ETA: 0s - loss: 12.3091 - accuracy: 0.252 - ETA: 0s - loss: 12.3187 - accuracy: 0.248 - ETA: 0s - loss: 13.6171 - accuracy: 0.235 - ETA: 0s - loss: 12.1161 - accuracy: 0.247 - ETA: 0s - loss: 11.6043 - accuracy: 0.240 - ETA: 0s - loss: 12.6703 - accuracy: 0.242 - ETA: 0s - loss: 11.8608 - accuracy: 0.238 - 1s 671us/step - loss: 11.8377 - accuracy: 0.2383\n",
      "Epoch 7/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 11.6564 - accuracy: 0.260 - ETA: 0s - loss: 7.0938 - accuracy: 0.286 - ETA: 0s - loss: 7.8674 - accuracy: 0.30 - ETA: 0s - loss: 6.9401 - accuracy: 0.29 - ETA: 0s - loss: 7.0338 - accuracy: 0.28 - ETA: 0s - loss: 36.5309 - accuracy: 0.270 - ETA: 0s - loss: 31.8286 - accuracy: 0.264 - ETA: 0s - loss: 28.0367 - accuracy: 0.269 - ETA: 0s - loss: 25.9311 - accuracy: 0.263 - 1s 745us/step - loss: 25.3743 - accuracy: 0.2635\n",
      "Epoch 8/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 56.2372 - accuracy: 0.200 - ETA: 0s - loss: 24.0040 - accuracy: 0.280 - ETA: 0s - loss: 17.2801 - accuracy: 0.280 - ETA: 0s - loss: 16.7090 - accuracy: 0.285 - ETA: 0s - loss: 27.6564 - accuracy: 0.284 - ETA: 0s - loss: 24.1820 - accuracy: 0.290 - ETA: 0s - loss: 23.3012 - accuracy: 0.283 - ETA: 0s - loss: 20.9450 - accuracy: 0.284 - ETA: 0s - loss: 20.3189 - accuracy: 0.278 - 1s 734us/step - loss: 20.0983 - accuracy: 0.2806\n",
      "Epoch 9/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 7.6952 - accuracy: 0.40 - ETA: 0s - loss: 9.0183 - accuracy: 0.32 - ETA: 0s - loss: 7.8904 - accuracy: 0.29 - ETA: 0s - loss: 7.1805 - accuracy: 0.28 - ETA: 0s - loss: 36.1646 - accuracy: 0.272 - ETA: 0s - loss: 30.3022 - accuracy: 0.268 - ETA: 0s - loss: 25.8727 - accuracy: 0.273 - ETA: 0s - loss: 22.8197 - accuracy: 0.274 - ETA: 0s - loss: 20.4997 - accuracy: 0.280 - 1s 788us/step - loss: 19.8747 - accuracy: 0.2806\n",
      "Epoch 10/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 12.4239 - accuracy: 0.360 - ETA: 0s - loss: 8.2126 - accuracy: 0.313 - ETA: 0s - loss: 5.7998 - accuracy: 0.32 - ETA: 0s - loss: 7.0716 - accuracy: 0.32 - ETA: 0s - loss: 10.0520 - accuracy: 0.292 - ETA: 0s - loss: 11.2553 - accuracy: 0.293 - ETA: 0s - loss: 11.6520 - accuracy: 0.290 - ETA: 0s - loss: 10.2937 - accuracy: 0.304 - ETA: 0s - loss: 9.6138 - accuracy: 0.293 - ETA: 0s - loss: 9.0387 - accuracy: 0.29 - 1s 789us/step - loss: 8.8793 - accuracy: 0.2967\n",
      "Epoch 11/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 6.0415 - accuracy: 0.20 - ETA: 0s - loss: 6.8492 - accuracy: 0.22 - ETA: 0s - loss: 5.6754 - accuracy: 0.23 - ETA: 0s - loss: 24.6168 - accuracy: 0.256 - ETA: 0s - loss: 18.6048 - accuracy: 0.288 - ETA: 0s - loss: 16.5215 - accuracy: 0.287 - ETA: 0s - loss: 15.9566 - accuracy: 0.297 - ETA: 0s - loss: 16.6939 - accuracy: 0.283 - ETA: 0s - loss: 14.6573 - accuracy: 0.290 - ETA: 0s - loss: 13.4247 - accuracy: 0.286 - ETA: 0s - loss: 12.3279 - accuracy: 0.285 - 1s 953us/step - loss: 12.1634 - accuracy: 0.2841\n",
      "Epoch 12/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 7.5850 - accuracy: 0.22 - ETA: 0s - loss: 5.5074 - accuracy: 0.26 - ETA: 0s - loss: 5.2926 - accuracy: 0.25 - ETA: 0s - loss: 5.1411 - accuracy: 0.28 - ETA: 0s - loss: 4.9804 - accuracy: 0.30 - ETA: 0s - loss: 6.0655 - accuracy: 0.29 - ETA: 0s - loss: 6.6537 - accuracy: 0.30 - ETA: 0s - loss: 8.2623 - accuracy: 0.29 - ETA: 0s - loss: 7.9724 - accuracy: 0.29 - ETA: 0s - loss: 7.8269 - accuracy: 0.30 - ETA: 0s - loss: 11.6773 - accuracy: 0.304 - 1s 916us/step - loss: 11.4305 - accuracy: 0.3047\n",
      "Epoch 13/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 3.7267 - accuracy: 0.30 - ETA: 0s - loss: 3.9609 - accuracy: 0.31 - ETA: 0s - loss: 23.0057 - accuracy: 0.300 - ETA: 0s - loss: 17.4910 - accuracy: 0.303 - ETA: 0s - loss: 15.5379 - accuracy: 0.300 - ETA: 0s - loss: 14.9011 - accuracy: 0.304 - ETA: 0s - loss: 13.4322 - accuracy: 0.307 - ETA: 0s - loss: 12.7360 - accuracy: 0.301 - ETA: 0s - loss: 11.3435 - accuracy: 0.308 - ETA: 0s - loss: 16.5137 - accuracy: 0.312 - ETA: 0s - loss: 15.2428 - accuracy: 0.320 - 1s 994us/step - loss: 14.9137 - accuracy: 0.3219\n",
      "Epoch 14/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 9.3087 - accuracy: 0.34 - ETA: 0s - loss: 6.2338 - accuracy: 0.28 - ETA: 0s - loss: 8.3698 - accuracy: 0.28 - ETA: 0s - loss: 11.4407 - accuracy: 0.283 - ETA: 0s - loss: 15.4421 - accuracy: 0.297 - ETA: 0s - loss: 13.4874 - accuracy: 0.297 - ETA: 0s - loss: 12.9393 - accuracy: 0.304 - ETA: 0s - loss: 11.2668 - accuracy: 0.308 - ETA: 0s - loss: 11.3426 - accuracy: 0.311 - ETA: 0s - loss: 13.4955 - accuracy: 0.308 - ETA: 0s - loss: 12.9673 - accuracy: 0.309 - 1s 1ms/step - loss: 12.7217 - accuracy: 0.3081\n",
      "Epoch 15/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 4.4115 - accuracy: 0.30 - ETA: 0s - loss: 6.2394 - accuracy: 0.33 - ETA: 0s - loss: 13.1751 - accuracy: 0.313 - ETA: 0s - loss: 16.4772 - accuracy: 0.312 - ETA: 0s - loss: 15.0672 - accuracy: 0.310 - ETA: 0s - loss: 13.2753 - accuracy: 0.305 - ETA: 0s - loss: 10.8923 - accuracy: 0.304 - ETA: 0s - loss: 9.9191 - accuracy: 0.294 - ETA: 0s - loss: 9.5455 - accuracy: 0.30 - ETA: 0s - loss: 14.1060 - accuracy: 0.307 - ETA: 0s - loss: 12.9904 - accuracy: 0.302 - 1s 885us/step - loss: 12.7965 - accuracy: 0.3047\n",
      "Epoch 16/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 4.8330 - accuracy: 0.34 - ETA: 0s - loss: 4.0857 - accuracy: 0.38 - ETA: 0s - loss: 3.9695 - accuracy: 0.35 - ETA: 0s - loss: 4.0257 - accuracy: 0.35 - ETA: 0s - loss: 3.9820 - accuracy: 0.35 - ETA: 0s - loss: 4.2113 - accuracy: 0.34 - ETA: 0s - loss: 4.1313 - accuracy: 0.34 - ETA: 0s - loss: 4.1003 - accuracy: 0.33 - ETA: 0s - loss: 4.3822 - accuracy: 0.32 - ETA: 0s - loss: 4.2416 - accuracy: 0.32 - ETA: 0s - loss: 5.0853 - accuracy: 0.33 - ETA: 0s - loss: 4.8988 - accuracy: 0.32 - ETA: 0s - loss: 9.4437 - accuracy: 0.32 - ETA: 0s - loss: 9.1393 - accuracy: 0.32 - 1s 1ms/step - loss: 8.9600 - accuracy: 0.3219\n",
      "Epoch 17/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 2.0645 - accuracy: 0.36 - ETA: 0s - loss: 2.5882 - accuracy: 0.39 - ETA: 0s - loss: 15.6713 - accuracy: 0.346 - ETA: 0s - loss: 12.6624 - accuracy: 0.310 - ETA: 0s - loss: 10.5322 - accuracy: 0.316 - ETA: 0s - loss: 11.1654 - accuracy: 0.320 - ETA: 0s - loss: 10.7118 - accuracy: 0.307 - ETA: 0s - loss: 9.7979 - accuracy: 0.306 - ETA: 0s - loss: 9.0719 - accuracy: 0.30 - ETA: 0s - loss: 20.8942 - accuracy: 0.314 - ETA: 0s - loss: 19.6056 - accuracy: 0.316 - ETA: 0s - loss: 18.3030 - accuracy: 0.309 - ETA: 0s - loss: 17.2483 - accuracy: 0.318 - ETA: 0s - loss: 16.4484 - accuracy: 0.318 - ETA: 0s - loss: 16.3067 - accuracy: 0.331 - 1s 1ms/step - loss: 15.9332 - accuracy: 0.3356\n",
      "Epoch 18/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 5.9393 - accuracy: 0.34 - ETA: 0s - loss: 4.3980 - accuracy: 0.34 - ETA: 0s - loss: 5.5173 - accuracy: 0.32 - ETA: 0s - loss: 4.6061 - accuracy: 0.31 - ETA: 0s - loss: 6.6803 - accuracy: 0.32 - ETA: 0s - loss: 8.0543 - accuracy: 0.34 - ETA: 0s - loss: 15.9364 - accuracy: 0.346 - ETA: 0s - loss: 16.3822 - accuracy: 0.350 - ETA: 0s - loss: 15.2873 - accuracy: 0.347 - ETA: 0s - loss: 14.6408 - accuracy: 0.357 - 1s 915us/step - loss: 14.3171 - accuracy: 0.3608\n",
      "Epoch 19/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 2.3109 - accuracy: 0.42 - ETA: 0s - loss: 5.1005 - accuracy: 0.36 - ETA: 0s - loss: 4.3843 - accuracy: 0.39 - ETA: 0s - loss: 4.4079 - accuracy: 0.37 - ETA: 0s - loss: 4.6336 - accuracy: 0.36 - ETA: 0s - loss: 4.9575 - accuracy: 0.36 - ETA: 0s - loss: 4.6808 - accuracy: 0.37 - ETA: 0s - loss: 5.1649 - accuracy: 0.36 - ETA: 0s - loss: 5.0301 - accuracy: 0.36 - ETA: 0s - loss: 4.7694 - accuracy: 0.36 - 1s 842us/step - loss: 4.6969 - accuracy: 0.3677\n",
      "Epoch 20/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 2.7203 - accuracy: 0.36 - ETA: 0s - loss: 3.0415 - accuracy: 0.32 - ETA: 0s - loss: 3.2717 - accuracy: 0.33 - ETA: 0s - loss: 3.1364 - accuracy: 0.33 - ETA: 0s - loss: 3.1535 - accuracy: 0.32 - ETA: 0s - loss: 2.8733 - accuracy: 0.33 - ETA: 0s - loss: 3.0020 - accuracy: 0.34 - ETA: 0s - loss: 18.4369 - accuracy: 0.360 - ETA: 0s - loss: 16.4163 - accuracy: 0.361 - ETA: 0s - loss: 14.5003 - accuracy: 0.366 - ETA: 0s - loss: 13.7472 - accuracy: 0.361 - 1s 942us/step - loss: 13.4578 - accuracy: 0.3597\n",
      "Epoch 21/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 1.8390 - accuracy: 0.32 - ETA: 0s - loss: 4.4774 - accuracy: 0.38 - ETA: 0s - loss: 3.9116 - accuracy: 0.34 - ETA: 0s - loss: 4.1609 - accuracy: 0.31 - ETA: 0s - loss: 4.1606 - accuracy: 0.31 - ETA: 0s - loss: 3.9242 - accuracy: 0.31 - ETA: 0s - loss: 3.9665 - accuracy: 0.32 - ETA: 0s - loss: 3.8274 - accuracy: 0.32 - ETA: 0s - loss: 4.6134 - accuracy: 0.33 - ETA: 0s - loss: 6.1940 - accuracy: 0.33 - 1s 854us/step - loss: 6.0114 - accuracy: 0.3345\n",
      "Epoch 22/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 2.1767 - accuracy: 0.38 - ETA: 0s - loss: 15.8622 - accuracy: 0.366 - ETA: 0s - loss: 12.5108 - accuracy: 0.365 - ETA: 0s - loss: 10.3802 - accuracy: 0.370 - ETA: 0s - loss: 8.5395 - accuracy: 0.357 - ETA: 0s - loss: 7.7734 - accuracy: 0.36 - ETA: 0s - loss: 7.2018 - accuracy: 0.36 - ETA: 0s - loss: 6.9567 - accuracy: 0.37 - ETA: 0s - loss: 6.3130 - accuracy: 0.36 - ETA: 0s - loss: 6.0744 - accuracy: 0.35 - 1s 864us/step - loss: 6.3421 - accuracy: 0.3608\n",
      "Epoch 23/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 2.6354 - accuracy: 0.40 - ETA: 0s - loss: 2.7738 - accuracy: 0.38 - ETA: 0s - loss: 2.5664 - accuracy: 0.36 - ETA: 0s - loss: 4.5671 - accuracy: 0.38 - ETA: 0s - loss: 5.5598 - accuracy: 0.37 - ETA: 0s - loss: 5.2335 - accuracy: 0.38 - ETA: 0s - loss: 4.7473 - accuracy: 0.36 - ETA: 0s - loss: 4.3829 - accuracy: 0.35 - ETA: 0s - loss: 4.1902 - accuracy: 0.36 - ETA: 0s - loss: 4.0175 - accuracy: 0.36 - 1s 828us/step - loss: 3.9586 - accuracy: 0.3597\n",
      "Epoch 24/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 2.5611 - accuracy: 0.36 - ETA: 0s - loss: 2.0927 - accuracy: 0.41 - ETA: 0s - loss: 2.2010 - accuracy: 0.41 - ETA: 0s - loss: 2.0844 - accuracy: 0.40 - ETA: 0s - loss: 2.0806 - accuracy: 0.40 - ETA: 0s - loss: 2.3152 - accuracy: 0.38 - ETA: 0s - loss: 2.3526 - accuracy: 0.39 - ETA: 0s - loss: 2.3870 - accuracy: 0.39 - ETA: 0s - loss: 2.3547 - accuracy: 0.38 - ETA: 0s - loss: 2.3219 - accuracy: 0.38 - 1s 868us/step - loss: 2.4950 - accuracy: 0.3849\n",
      "Epoch 25/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 1.4926 - accuracy: 0.44 - ETA: 0s - loss: 1.6200 - accuracy: 0.40 - ETA: 0s - loss: 1.9568 - accuracy: 0.40 - ETA: 0s - loss: 2.1332 - accuracy: 0.38 - ETA: 0s - loss: 2.4101 - accuracy: 0.38 - ETA: 0s - loss: 2.2726 - accuracy: 0.39 - ETA: 0s - loss: 2.2047 - accuracy: 0.39 - ETA: 0s - loss: 2.2223 - accuracy: 0.39 - ETA: 0s - loss: 2.2964 - accuracy: 0.39 - 1s 780us/step - loss: 2.2877 - accuracy: 0.3952\n",
      "Epoch 26/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 7.5263 - accuracy: 0.52 - ETA: 0s - loss: 4.5162 - accuracy: 0.47 - ETA: 0s - loss: 3.1260 - accuracy: 0.43 - ETA: 0s - loss: 2.6982 - accuracy: 0.41 - ETA: 0s - loss: 2.4770 - accuracy: 0.41 - ETA: 0s - loss: 19.9861 - accuracy: 0.395 - ETA: 0s - loss: 16.8597 - accuracy: 0.385 - ETA: 0s - loss: 14.6134 - accuracy: 0.389 - ETA: 0s - loss: 13.3333 - accuracy: 0.385 - ETA: 0s - loss: 12.4623 - accuracy: 0.387 - 1s 833us/step - loss: 12.1759 - accuracy: 0.3895\n",
      "Epoch 27/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 2.7048 - accuracy: 0.40 - ETA: 0s - loss: 2.0020 - accuracy: 0.38 - ETA: 0s - loss: 2.6218 - accuracy: 0.42 - ETA: 0s - loss: 2.5441 - accuracy: 0.41 - ETA: 0s - loss: 2.4222 - accuracy: 0.40 - ETA: 0s - loss: 2.6250 - accuracy: 0.40 - ETA: 0s - loss: 3.4082 - accuracy: 0.39 - ETA: 0s - loss: 3.2514 - accuracy: 0.39 - ETA: 0s - loss: 3.4174 - accuracy: 0.38 - 1s 720us/step - loss: 3.3929 - accuracy: 0.3860\n",
      "Epoch 28/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 1.9854 - accuracy: 0.28 - ETA: 0s - loss: 2.3666 - accuracy: 0.30 - ETA: 0s - loss: 2.2308 - accuracy: 0.34 - ETA: 0s - loss: 2.1857 - accuracy: 0.36 - ETA: 0s - loss: 2.1067 - accuracy: 0.37 - ETA: 0s - loss: 2.4106 - accuracy: 0.37 - ETA: 0s - loss: 2.5349 - accuracy: 0.36 - ETA: 0s - loss: 2.5514 - accuracy: 0.35 - ETA: 0s - loss: 2.5074 - accuracy: 0.36 - 1s 734us/step - loss: 2.5053 - accuracy: 0.3620\n",
      "Epoch 29/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 1.5920 - accuracy: 0.40 - ETA: 0s - loss: 1.7892 - accuracy: 0.42 - ETA: 0s - loss: 3.0796 - accuracy: 0.42 - ETA: 0s - loss: 2.8190 - accuracy: 0.43 - ETA: 0s - loss: 3.2184 - accuracy: 0.39 - ETA: 0s - loss: 2.9334 - accuracy: 0.39 - ETA: 0s - loss: 2.7137 - accuracy: 0.40 - ETA: 0s - loss: 2.5861 - accuracy: 0.39 - ETA: 0s - loss: 2.5565 - accuracy: 0.39 - ETA: 0s - loss: 2.6702 - accuracy: 0.39 - 1s 851us/step - loss: 2.6514 - accuracy: 0.3906\n",
      "Epoch 30/30\n",
      "873/873 [==============================] - ETA: 0s - loss: 1.9319 - accuracy: 0.36 - ETA: 0s - loss: 1.7282 - accuracy: 0.42 - ETA: 0s - loss: 1.8607 - accuracy: 0.41 - ETA: 0s - loss: 1.7962 - accuracy: 0.40 - ETA: 0s - loss: 1.7565 - accuracy: 0.41 - ETA: 0s - loss: 1.7729 - accuracy: 0.40 - ETA: 0s - loss: 1.7697 - accuracy: 0.41 - ETA: 0s - loss: 1.7971 - accuracy: 0.42 - ETA: 0s - loss: 1.8030 - accuracy: 0.42 - ETA: 0s - loss: 1.8034 - accuracy: 0.42 - 1s 881us/step - loss: 1.7925 - accuracy: 0.4215\n"
     ]
    }
   ],
   "source": [
    "# Model from GitHub\n",
    "\n",
    "#adding layers and forming the model\n",
    "\n",
    "#forming model\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=5,strides=1,padding=\"Same\",activation=\"relu\",input_shape=(60,4,1)))\n",
    "model.add(MaxPooling2D(padding=\"same\"))\n",
    "\n",
    "model.add(Conv2D(128,kernel_size=5,strides=1,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(padding=\"same\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256,activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(512,activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(10,activation=\"softmax\"))\n",
    "\n",
    "#compiling\n",
    "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "#training the model\n",
    "model_history = model.fit(X_train[0],Y_train[0],batch_size=50,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
